{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load prepared dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape: (32359, 50)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Countries</th>\n",
       "      <th>Sand_1</th>\n",
       "      <th>Sand_2</th>\n",
       "      <th>Sand_3</th>\n",
       "      <th>Sand_4</th>\n",
       "      <th>Sand_5</th>\n",
       "      <th>Sand_6</th>\n",
       "      <th>Sand_7</th>\n",
       "      <th>Clay_1</th>\n",
       "      <th>Clay_2</th>\n",
       "      <th>Clay_3</th>\n",
       "      <th>Clay_4</th>\n",
       "      <th>Clay_5</th>\n",
       "      <th>Clay_6</th>\n",
       "      <th>Clay_7</th>\n",
       "      <th>OC_1</th>\n",
       "      <th>OC_2</th>\n",
       "      <th>OC_3</th>\n",
       "      <th>OC_4</th>\n",
       "      <th>OC_5</th>\n",
       "      <th>OC_6</th>\n",
       "      <th>OC_7</th>\n",
       "      <th>PAW_1</th>\n",
       "      <th>PAW_2</th>\n",
       "      <th>PAW_3</th>\n",
       "      <th>PAW_4</th>\n",
       "      <th>PAW_5</th>\n",
       "      <th>PAW_6</th>\n",
       "      <th>PAW_7</th>\n",
       "      <th>Y_maize_major</th>\n",
       "      <th>Farm</th>\n",
       "      <th>Sow_Maize_month_int</th>\n",
       "      <th>Harvest_Maize_month_int</th>\n",
       "      <th>sow_to_harvest_months</th>\n",
       "      <th>maize_lag-1</th>\n",
       "      <th>pcp_mean_lag-1</th>\n",
       "      <th>tmax_mean_lag-1</th>\n",
       "      <th>tmin_mean_lag-1</th>\n",
       "      <th>spi_mean_lag-1</th>\n",
       "      <th>maize_lag-2</th>\n",
       "      <th>pcp_mean_lag-2</th>\n",
       "      <th>tmax_mean_lag-2</th>\n",
       "      <th>tmin_mean_lag-2</th>\n",
       "      <th>spi_mean_lag-2</th>\n",
       "      <th>maize_lag-3</th>\n",
       "      <th>pcp_mean_lag-3</th>\n",
       "      <th>tmax_mean_lag-3</th>\n",
       "      <th>tmin_mean_lag-3</th>\n",
       "      <th>spi_mean_lag-3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007</td>\n",
       "      <td>Angola</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>48</td>\n",
       "      <td>45</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>37</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.615357</td>\n",
       "      <td>104_Angola</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.554392</td>\n",
       "      <td>97.103755</td>\n",
       "      <td>301.939623</td>\n",
       "      <td>292.214020</td>\n",
       "      <td>0.093447</td>\n",
       "      <td>0.721607</td>\n",
       "      <td>129.051864</td>\n",
       "      <td>301.518536</td>\n",
       "      <td>292.496579</td>\n",
       "      <td>1.644698</td>\n",
       "      <td>0.620005</td>\n",
       "      <td>109.983325</td>\n",
       "      <td>301.786056</td>\n",
       "      <td>292.204097</td>\n",
       "      <td>0.514275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007</td>\n",
       "      <td>Angola</td>\n",
       "      <td>62</td>\n",
       "      <td>64</td>\n",
       "      <td>63</td>\n",
       "      <td>59</td>\n",
       "      <td>58</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.257656</td>\n",
       "      <td>99_Angola</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.117051</td>\n",
       "      <td>59.292237</td>\n",
       "      <td>301.882929</td>\n",
       "      <td>288.092753</td>\n",
       "      <td>0.182926</td>\n",
       "      <td>0.300217</td>\n",
       "      <td>47.697564</td>\n",
       "      <td>303.988747</td>\n",
       "      <td>288.916992</td>\n",
       "      <td>0.909295</td>\n",
       "      <td>0.212699</td>\n",
       "      <td>41.130026</td>\n",
       "      <td>303.298082</td>\n",
       "      <td>288.642853</td>\n",
       "      <td>0.588172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007</td>\n",
       "      <td>Angola</td>\n",
       "      <td>69</td>\n",
       "      <td>71</td>\n",
       "      <td>70</td>\n",
       "      <td>67</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.286831</td>\n",
       "      <td>108_Angola</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3.093239</td>\n",
       "      <td>58.196545</td>\n",
       "      <td>302.891420</td>\n",
       "      <td>289.377311</td>\n",
       "      <td>0.991663</td>\n",
       "      <td>4.044452</td>\n",
       "      <td>42.130629</td>\n",
       "      <td>305.494178</td>\n",
       "      <td>290.535403</td>\n",
       "      <td>0.952237</td>\n",
       "      <td>2.295351</td>\n",
       "      <td>35.049776</td>\n",
       "      <td>304.824778</td>\n",
       "      <td>290.284886</td>\n",
       "      <td>0.371446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007</td>\n",
       "      <td>Angola</td>\n",
       "      <td>60</td>\n",
       "      <td>63</td>\n",
       "      <td>61</td>\n",
       "      <td>57</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.700384</td>\n",
       "      <td>102_Angola</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.677797</td>\n",
       "      <td>149.210195</td>\n",
       "      <td>298.973795</td>\n",
       "      <td>287.311403</td>\n",
       "      <td>0.206751</td>\n",
       "      <td>0.907431</td>\n",
       "      <td>159.454723</td>\n",
       "      <td>299.404975</td>\n",
       "      <td>287.724299</td>\n",
       "      <td>1.374616</td>\n",
       "      <td>0.783018</td>\n",
       "      <td>174.088260</td>\n",
       "      <td>298.908208</td>\n",
       "      <td>287.362407</td>\n",
       "      <td>0.643207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007</td>\n",
       "      <td>Angola</td>\n",
       "      <td>67</td>\n",
       "      <td>69</td>\n",
       "      <td>68</td>\n",
       "      <td>63</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.553450</td>\n",
       "      <td>43_Angola</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.412071</td>\n",
       "      <td>74.556629</td>\n",
       "      <td>304.006860</td>\n",
       "      <td>290.606725</td>\n",
       "      <td>-0.075621</td>\n",
       "      <td>0.675967</td>\n",
       "      <td>66.698670</td>\n",
       "      <td>304.644632</td>\n",
       "      <td>290.635254</td>\n",
       "      <td>1.144088</td>\n",
       "      <td>0.605584</td>\n",
       "      <td>67.404588</td>\n",
       "      <td>303.930955</td>\n",
       "      <td>290.564185</td>\n",
       "      <td>0.553079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year Countries  Sand_1  Sand_2  Sand_3  Sand_4  Sand_5  Sand_6  Sand_7  \\\n",
       "0  2007    Angola      50      51      51      48      45      46      46   \n",
       "1  2007    Angola      62      64      63      59      58      59      59   \n",
       "2  2007    Angola      69      71      70      67      65      65      66   \n",
       "3  2007    Angola      60      63      61      57      53      53      53   \n",
       "4  2007    Angola      67      69      68      63      61      61      61   \n",
       "\n",
       "   Clay_1  Clay_2  Clay_3  Clay_4  Clay_5  Clay_6  Clay_7  OC_1  OC_2  OC_3  \\\n",
       "0      37      35      36      39      42      42      42  0.52  0.23  0.17   \n",
       "1      27      25      26      29      31      30      30  0.11  0.05  0.07   \n",
       "2      19      16      18      21      24      24      23  0.09  0.06  0.07   \n",
       "3      29      26      28      32      35      36      36  0.46  0.16  0.14   \n",
       "4      22      19      21      25      28      28      29  0.15  0.09  0.09   \n",
       "\n",
       "   OC_4  OC_5  OC_6  OC_7  PAW_1  PAW_2  PAW_3  PAW_4  PAW_5  PAW_6  PAW_7  \\\n",
       "0  0.09  0.04  0.02  0.02   0.15   0.15   0.14   0.13   0.10   0.07   0.07   \n",
       "1  0.04  0.02  0.02  0.01   0.11   0.10   0.10   0.09   0.07   0.07   0.03   \n",
       "2  0.04  0.02  0.02  0.02   0.10   0.10   0.10   0.09   0.07   0.07   0.07   \n",
       "3  0.08  0.05  0.04  0.03   0.12   0.13   0.12   0.12   0.11   0.10   0.09   \n",
       "4  0.05  0.02  0.01  0.01   0.11   0.11   0.11   0.11   0.08   0.04   0.04   \n",
       "\n",
       "   Y_maize_major        Farm  Sow_Maize_month_int  Harvest_Maize_month_int  \\\n",
       "0       0.615357  104_Angola                    9                        4   \n",
       "1       0.257656   99_Angola                    9                        4   \n",
       "2       4.286831  108_Angola                    9                        4   \n",
       "3       0.700384  102_Angola                    9                        4   \n",
       "4       0.553450   43_Angola                    9                        4   \n",
       "\n",
       "   sow_to_harvest_months  maize_lag-1  pcp_mean_lag-1  tmax_mean_lag-1  \\\n",
       "0                      7     0.554392       97.103755       301.939623   \n",
       "1                      7     0.117051       59.292237       301.882929   \n",
       "2                      7     3.093239       58.196545       302.891420   \n",
       "3                      7     0.677797      149.210195       298.973795   \n",
       "4                      7     0.412071       74.556629       304.006860   \n",
       "\n",
       "   tmin_mean_lag-1  spi_mean_lag-1  maize_lag-2  pcp_mean_lag-2  \\\n",
       "0       292.214020        0.093447     0.721607      129.051864   \n",
       "1       288.092753        0.182926     0.300217       47.697564   \n",
       "2       289.377311        0.991663     4.044452       42.130629   \n",
       "3       287.311403        0.206751     0.907431      159.454723   \n",
       "4       290.606725       -0.075621     0.675967       66.698670   \n",
       "\n",
       "   tmax_mean_lag-2  tmin_mean_lag-2  spi_mean_lag-2  maize_lag-3  \\\n",
       "0       301.518536       292.496579        1.644698     0.620005   \n",
       "1       303.988747       288.916992        0.909295     0.212699   \n",
       "2       305.494178       290.535403        0.952237     2.295351   \n",
       "3       299.404975       287.724299        1.374616     0.783018   \n",
       "4       304.644632       290.635254        1.144088     0.605584   \n",
       "\n",
       "   pcp_mean_lag-3  tmax_mean_lag-3  tmin_mean_lag-3  spi_mean_lag-3  \n",
       "0      109.983325       301.786056       292.204097        0.514275  \n",
       "1       41.130026       303.298082       288.642853        0.588172  \n",
       "2       35.049776       304.824778       290.284886        0.371446  \n",
       "3      174.088260       298.908208       287.362407        0.643207  \n",
       "4       67.404588       303.930955       290.564185        0.553079  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/df_prepped.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print('df.shape:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Countries.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3887"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Farm.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year                       0\n",
       "Countries                  0\n",
       "Sand_1                     0\n",
       "Sand_2                     0\n",
       "Sand_3                     0\n",
       "Sand_4                     0\n",
       "Sand_5                     0\n",
       "Sand_6                     0\n",
       "Sand_7                     0\n",
       "Clay_1                     0\n",
       "Clay_2                     0\n",
       "Clay_3                     0\n",
       "Clay_4                     0\n",
       "Clay_5                     0\n",
       "Clay_6                     0\n",
       "Clay_7                     0\n",
       "OC_1                       0\n",
       "OC_2                       0\n",
       "OC_3                       0\n",
       "OC_4                       0\n",
       "OC_5                       0\n",
       "OC_6                       0\n",
       "OC_7                       0\n",
       "PAW_1                      0\n",
       "PAW_2                      0\n",
       "PAW_3                      0\n",
       "PAW_4                      0\n",
       "PAW_5                      0\n",
       "PAW_6                      0\n",
       "PAW_7                      0\n",
       "Y_maize_major              0\n",
       "Farm                       0\n",
       "Sow_Maize_month_int        0\n",
       "Harvest_Maize_month_int    0\n",
       "sow_to_harvest_months      0\n",
       "maize_lag-1                0\n",
       "pcp_mean_lag-1             0\n",
       "tmax_mean_lag-1            0\n",
       "tmin_mean_lag-1            0\n",
       "spi_mean_lag-1             0\n",
       "maize_lag-2                0\n",
       "pcp_mean_lag-2             0\n",
       "tmax_mean_lag-2            0\n",
       "tmin_mean_lag-2            0\n",
       "spi_mean_lag-2             0\n",
       "maize_lag-3                0\n",
       "pcp_mean_lag-3             0\n",
       "tmax_mean_lag-3            0\n",
       "tmin_mean_lag-3            0\n",
       "spi_mean_lag-3             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping countries and farms because one-hot-encoding would make 4000+ rows\n",
    "# We tested that and found that it did not improve performance but increased runtime\n",
    "df_label = df.loc[:,['Countries','Farm']]\n",
    "df = df.drop(['Countries','Farm'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate a test set, the year 2016\n",
    "test = df[df.Year == 2016]\n",
    "df_train = df[df.Year < 2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set has years:  [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]\n",
      "The test set has years:  [2016]\n"
     ]
    }
   ],
   "source": [
    "print('The training set has years: ', list(df_train.Year.unique()))\n",
    "print('The test set has years: ', list(test.Year.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline selection using CV and years 2007-2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  [2007, 2008, 2009, 2010] VAL:  [2011]\n",
      "TRAIN:  [2007, 2008, 2009, 2010, 2011] VAL:  [2012]\n",
      "TRAIN:  [2007, 2008, 2009, 2010, 2011, 2012] VAL:  [2013]\n",
      "TRAIN:  [2007, 2008, 2009, 2010, 2011, 2012, 2013] VAL:  [2014]\n",
      "TRAIN:  [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014] VAL:  [2015]\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation set-up\n",
    "# https://stackoverflow.com/questions/58069691/how-to-create-a-train-test-split-of-time-series-data-by-year\n",
    "\n",
    "year_list = df_train['Year'].unique().tolist()\n",
    "splits = {'train': [], 'val': []}\n",
    "\n",
    "for idx, yr in enumerate(year_list[:-1]):\n",
    "    if yr < 2010:\n",
    "        # To get only the last 5 splits\n",
    "        continue\n",
    "    train_yr = year_list[:idx+1]\n",
    "    test_yr = [year_list[idx+1]]\n",
    "    print('TRAIN: ', train_yr, 'VAL: ',test_yr)\n",
    "    \n",
    "    splits['train'].append(df_train.loc[df_train.Year.isin(train_yr), :])\n",
    "    splits['val'].append(df_train.loc[df_train.Year.isin(test_yr), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = {\n",
    "    \"KNN\": KNeighborsRegressor(), # 1 cannot extrapolate\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=SEED), # 2 cannot extrapolate\n",
    "    \"Gradient Boosting\": AdaBoostRegressor(random_state=SEED), # 3 cannot extrapolate\n",
    "    \"Linear Regression\": LinearRegression(), # 4 can extrapolate\n",
    "    \"LGBM\": lgb.sklearn.LGBMRegressor(random_state=SEED) # 5 can extrapolate\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "RMSE: 0.44075591508811046\n",
      "MAE: 0.2936247010209556\n",
      "Random Forest\n",
      "RMSE: 0.35625256905189046\n",
      "MAE: 0.23214587564870676\n",
      "Gradient Boosting\n",
      "RMSE: 0.4875346227174587\n",
      "MAE: 0.3971658820206708\n",
      "Linear Regression\n",
      "RMSE: 0.3802067681736373\n",
      "MAE: 0.2581393745105981\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5314\n",
      "[LightGBM] [Info] Number of data points in the train set: 15512, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score 1.636864\n",
      "LGBM\n",
      "RMSE: 0.40319203965072503\n",
      "MAE: 0.2548949593684437\n",
      "###\n",
      "\n",
      "Fold:  2\n",
      "KNN\n",
      "RMSE: 0.45573247287781565\n",
      "MAE: 0.29140229382129695\n",
      "Random Forest\n",
      "RMSE: 0.3814279850259372\n",
      "MAE: 0.22931856221496164\n",
      "Gradient Boosting\n",
      "RMSE: 0.5103637162242142\n",
      "MAE: 0.3757062501105622\n",
      "Linear Regression\n",
      "RMSE: 0.4125156152014488\n",
      "MAE: 0.252363280898665\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004834 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5314\n",
      "[LightGBM] [Info] Number of data points in the train set: 18442, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score 1.660769\n",
      "LGBM\n",
      "RMSE: 0.4236863617871984\n",
      "MAE: 0.2526193986004649\n",
      "###\n",
      "\n",
      "Fold:  3\n",
      "KNN\n",
      "RMSE: 0.4172097699242356\n",
      "MAE: 0.2845728520103754\n",
      "Random Forest\n",
      "RMSE: 0.3540032452220589\n",
      "MAE: 0.254877239764125\n",
      "Gradient Boosting\n",
      "RMSE: 0.40010071107261885\n",
      "MAE: 0.3171720956208696\n",
      "Linear Regression\n",
      "RMSE: 0.2925775246842222\n",
      "MAE: 0.20836135811268874\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006473 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5314\n",
      "[LightGBM] [Info] Number of data points in the train set: 21372, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score 1.670143\n",
      "LGBM\n",
      "RMSE: 0.3630187071678713\n",
      "MAE: 0.25019640415538114\n",
      "###\n",
      "\n",
      "Fold:  4\n",
      "KNN\n",
      "RMSE: 0.4776906501860052\n",
      "MAE: 0.3096619629464846\n",
      "Random Forest\n",
      "RMSE: 0.4321067686172712\n",
      "MAE: 0.2407240627447406\n",
      "Gradient Boosting\n",
      "RMSE: 0.4659205429378286\n",
      "MAE: 0.3275729776587513\n",
      "Linear Regression\n",
      "RMSE: 0.3727533546196708\n",
      "MAE: 0.2153534613234938\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005777 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5314\n",
      "[LightGBM] [Info] Number of data points in the train set: 24302, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score 1.684543\n",
      "LGBM\n",
      "RMSE: 0.42320414847895027\n",
      "MAE: 0.23131013338796996\n",
      "###\n",
      "\n",
      "Fold:  5\n",
      "KNN\n",
      "RMSE: 0.5585115966916876\n",
      "MAE: 0.3331649307034137\n",
      "Random Forest\n",
      "RMSE: 0.5309698856585617\n",
      "MAE: 0.2711682360286059\n",
      "Gradient Boosting\n",
      "RMSE: 0.6311921720471594\n",
      "MAE: 0.44624266470660184\n",
      "Linear Regression\n",
      "RMSE: 0.49940768044011596\n",
      "MAE: 0.261106978365429\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006885 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5314\n",
      "[LightGBM] [Info] Number of data points in the train set: 27232, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score 1.708814\n",
      "LGBM\n",
      "RMSE: 0.5154731434526998\n",
      "MAE: 0.26443612012630935\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This will take about 15 minutes\n",
    "\n",
    "reg_names = []\n",
    "cv_MAE = []\n",
    "cv_RMSE = []\n",
    "cv_folds = []\n",
    "train_years = []\n",
    "val_year = []\n",
    "\n",
    "i = 1\n",
    "for train, val in zip(splits['train'], splits['val']):\n",
    "\n",
    "    print('Fold: ', i)\n",
    "\n",
    "    # Shuffle\n",
    "    train = train.sample(frac=1, random_state=SEED)\n",
    "    val = val.sample(frac=1, random_state=SEED)\n",
    "    \n",
    "    # X and y\n",
    "    X_train = train.drop(columns=['Y_maize_major','Year'], axis=1)\n",
    "    y_train = train['Y_maize_major']\n",
    "    X_val = val.drop(columns=['Y_maize_major','Year'], axis=1)\n",
    "    y_val = val['Y_maize_major']\n",
    "\n",
    "    # Scale to [0,1] range\n",
    "    sc = MinMaxScaler()\n",
    "    X_train = pd.DataFrame(sc.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_val = pd.DataFrame(sc.transform(X_val), columns=X_val.columns)\n",
    "\n",
    "    # Fit and predict\n",
    "    for reg_name, reg in regressors.items():\n",
    "        reg_names.append(reg_name)\n",
    "        reg.fit(X_train, y_train)\n",
    "        y_val_pred = reg.predict(X_val)\n",
    "        rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "        mae = mean_absolute_error(y_val, y_val_pred)\n",
    "        cv_MAE.append(mae)\n",
    "        cv_RMSE.append(rmse)\n",
    "        cv_folds.append(i)\n",
    "        train_years.append(list(train.Year.unique()))\n",
    "        val_year.append(list(val.Year.unique()))\n",
    "\n",
    "        print(reg_name)\n",
    "        print('RMSE:', rmse)\n",
    "        print('MAE:', mae)\n",
    "\n",
    "    i += 1\n",
    "    print('###')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>regressor</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>train_years</th>\n",
       "      <th>val_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.440756</td>\n",
       "      <td>0.293625</td>\n",
       "      <td>[2008, 2007, 2010, 2009]</td>\n",
       "      <td>[2011]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.356253</td>\n",
       "      <td>0.232146</td>\n",
       "      <td>[2008, 2007, 2010, 2009]</td>\n",
       "      <td>[2011]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.487535</td>\n",
       "      <td>0.397166</td>\n",
       "      <td>[2008, 2007, 2010, 2009]</td>\n",
       "      <td>[2011]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.380207</td>\n",
       "      <td>0.258139</td>\n",
       "      <td>[2008, 2007, 2010, 2009]</td>\n",
       "      <td>[2011]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.403192</td>\n",
       "      <td>0.254895</td>\n",
       "      <td>[2008, 2007, 2010, 2009]</td>\n",
       "      <td>[2011]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.455732</td>\n",
       "      <td>0.291402</td>\n",
       "      <td>[2009, 2010, 2007, 2008, 2011]</td>\n",
       "      <td>[2012]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.381428</td>\n",
       "      <td>0.229319</td>\n",
       "      <td>[2009, 2010, 2007, 2008, 2011]</td>\n",
       "      <td>[2012]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.510364</td>\n",
       "      <td>0.375706</td>\n",
       "      <td>[2009, 2010, 2007, 2008, 2011]</td>\n",
       "      <td>[2012]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.412516</td>\n",
       "      <td>0.252363</td>\n",
       "      <td>[2009, 2010, 2007, 2008, 2011]</td>\n",
       "      <td>[2012]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.423686</td>\n",
       "      <td>0.252619</td>\n",
       "      <td>[2009, 2010, 2007, 2008, 2011]</td>\n",
       "      <td>[2012]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.417210</td>\n",
       "      <td>0.284573</td>\n",
       "      <td>[2012, 2010, 2009, 2008, 2011, 2007]</td>\n",
       "      <td>[2013]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.354003</td>\n",
       "      <td>0.254877</td>\n",
       "      <td>[2012, 2010, 2009, 2008, 2011, 2007]</td>\n",
       "      <td>[2013]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.400101</td>\n",
       "      <td>0.317172</td>\n",
       "      <td>[2012, 2010, 2009, 2008, 2011, 2007]</td>\n",
       "      <td>[2013]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.292578</td>\n",
       "      <td>0.208361</td>\n",
       "      <td>[2012, 2010, 2009, 2008, 2011, 2007]</td>\n",
       "      <td>[2013]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.363019</td>\n",
       "      <td>0.250196</td>\n",
       "      <td>[2012, 2010, 2009, 2008, 2011, 2007]</td>\n",
       "      <td>[2013]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.477691</td>\n",
       "      <td>0.309662</td>\n",
       "      <td>[2007, 2008, 2012, 2013, 2011, 2010, 2009]</td>\n",
       "      <td>[2014]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.432107</td>\n",
       "      <td>0.240724</td>\n",
       "      <td>[2007, 2008, 2012, 2013, 2011, 2010, 2009]</td>\n",
       "      <td>[2014]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.465921</td>\n",
       "      <td>0.327573</td>\n",
       "      <td>[2007, 2008, 2012, 2013, 2011, 2010, 2009]</td>\n",
       "      <td>[2014]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.372753</td>\n",
       "      <td>0.215353</td>\n",
       "      <td>[2007, 2008, 2012, 2013, 2011, 2010, 2009]</td>\n",
       "      <td>[2014]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.423204</td>\n",
       "      <td>0.231310</td>\n",
       "      <td>[2007, 2008, 2012, 2013, 2011, 2010, 2009]</td>\n",
       "      <td>[2014]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.558512</td>\n",
       "      <td>0.333165</td>\n",
       "      <td>[2008, 2009, 2011, 2007, 2013, 2012, 2010, 2014]</td>\n",
       "      <td>[2015]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.530970</td>\n",
       "      <td>0.271168</td>\n",
       "      <td>[2008, 2009, 2011, 2007, 2013, 2012, 2010, 2014]</td>\n",
       "      <td>[2015]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.631192</td>\n",
       "      <td>0.446243</td>\n",
       "      <td>[2008, 2009, 2011, 2007, 2013, 2012, 2010, 2014]</td>\n",
       "      <td>[2015]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.499408</td>\n",
       "      <td>0.261107</td>\n",
       "      <td>[2008, 2009, 2011, 2007, 2013, 2012, 2010, 2014]</td>\n",
       "      <td>[2015]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5</td>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.515473</td>\n",
       "      <td>0.264436</td>\n",
       "      <td>[2008, 2009, 2011, 2007, 2013, 2012, 2010, 2014]</td>\n",
       "      <td>[2015]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fold          regressor      RMSE       MAE  \\\n",
       "0      1                KNN  0.440756  0.293625   \n",
       "1      1      Random Forest  0.356253  0.232146   \n",
       "2      1  Gradient Boosting  0.487535  0.397166   \n",
       "3      1  Linear Regression  0.380207  0.258139   \n",
       "4      1               LGBM  0.403192  0.254895   \n",
       "5      2                KNN  0.455732  0.291402   \n",
       "6      2      Random Forest  0.381428  0.229319   \n",
       "7      2  Gradient Boosting  0.510364  0.375706   \n",
       "8      2  Linear Regression  0.412516  0.252363   \n",
       "9      2               LGBM  0.423686  0.252619   \n",
       "10     3                KNN  0.417210  0.284573   \n",
       "11     3      Random Forest  0.354003  0.254877   \n",
       "12     3  Gradient Boosting  0.400101  0.317172   \n",
       "13     3  Linear Regression  0.292578  0.208361   \n",
       "14     3               LGBM  0.363019  0.250196   \n",
       "15     4                KNN  0.477691  0.309662   \n",
       "16     4      Random Forest  0.432107  0.240724   \n",
       "17     4  Gradient Boosting  0.465921  0.327573   \n",
       "18     4  Linear Regression  0.372753  0.215353   \n",
       "19     4               LGBM  0.423204  0.231310   \n",
       "20     5                KNN  0.558512  0.333165   \n",
       "21     5      Random Forest  0.530970  0.271168   \n",
       "22     5  Gradient Boosting  0.631192  0.446243   \n",
       "23     5  Linear Regression  0.499408  0.261107   \n",
       "24     5               LGBM  0.515473  0.264436   \n",
       "\n",
       "                                         train_years val_year  \n",
       "0                           [2008, 2007, 2010, 2009]   [2011]  \n",
       "1                           [2008, 2007, 2010, 2009]   [2011]  \n",
       "2                           [2008, 2007, 2010, 2009]   [2011]  \n",
       "3                           [2008, 2007, 2010, 2009]   [2011]  \n",
       "4                           [2008, 2007, 2010, 2009]   [2011]  \n",
       "5                     [2009, 2010, 2007, 2008, 2011]   [2012]  \n",
       "6                     [2009, 2010, 2007, 2008, 2011]   [2012]  \n",
       "7                     [2009, 2010, 2007, 2008, 2011]   [2012]  \n",
       "8                     [2009, 2010, 2007, 2008, 2011]   [2012]  \n",
       "9                     [2009, 2010, 2007, 2008, 2011]   [2012]  \n",
       "10              [2012, 2010, 2009, 2008, 2011, 2007]   [2013]  \n",
       "11              [2012, 2010, 2009, 2008, 2011, 2007]   [2013]  \n",
       "12              [2012, 2010, 2009, 2008, 2011, 2007]   [2013]  \n",
       "13              [2012, 2010, 2009, 2008, 2011, 2007]   [2013]  \n",
       "14              [2012, 2010, 2009, 2008, 2011, 2007]   [2013]  \n",
       "15        [2007, 2008, 2012, 2013, 2011, 2010, 2009]   [2014]  \n",
       "16        [2007, 2008, 2012, 2013, 2011, 2010, 2009]   [2014]  \n",
       "17        [2007, 2008, 2012, 2013, 2011, 2010, 2009]   [2014]  \n",
       "18        [2007, 2008, 2012, 2013, 2011, 2010, 2009]   [2014]  \n",
       "19        [2007, 2008, 2012, 2013, 2011, 2010, 2009]   [2014]  \n",
       "20  [2008, 2009, 2011, 2007, 2013, 2012, 2010, 2014]   [2015]  \n",
       "21  [2008, 2009, 2011, 2007, 2013, 2012, 2010, 2014]   [2015]  \n",
       "22  [2008, 2009, 2011, 2007, 2013, 2012, 2010, 2014]   [2015]  \n",
       "23  [2008, 2009, 2011, 2007, 2013, 2012, 2010, 2014]   [2015]  \n",
       "24  [2008, 2009, 2011, 2007, 2013, 2012, 2010, 2014]   [2015]  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame()\n",
    "df_results['fold'] = cv_folds\n",
    "df_results['regressor'] = reg_names\n",
    "df_results['RMSE'] = cv_RMSE\n",
    "df_results['MAE'] = cv_MAE\n",
    "df_results['train_years'] = train_years\n",
    "df_results['val_year'] = val_year\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv('../experiment_results/baseline_selection_results_Kea.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regressor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.499022</td>\n",
       "      <td>0.372772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.469980</td>\n",
       "      <td>0.302485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM</th>\n",
       "      <td>0.425715</td>\n",
       "      <td>0.250691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Regression</th>\n",
       "      <td>0.391492</td>\n",
       "      <td>0.239065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.410952</td>\n",
       "      <td>0.245647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       RMSE       MAE\n",
       "                       mean      mean\n",
       "regressor                            \n",
       "Gradient Boosting  0.499022  0.372772\n",
       "KNN                0.469980  0.302485\n",
       "LGBM               0.425715  0.250691\n",
       "Linear Regression  0.391492  0.239065\n",
       "Random Forest      0.410952  0.245647"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_agg = df_results.groupby('regressor').agg({'RMSE': ['mean'], 'MAE': [ 'mean']})\n",
    "df_results_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is has lowest RMSE and MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to find from results mean RMSE for Random Forest \n",
    "mean_RMSE_best = df_results[df_results['regressor'] == 'Linear Regression'].agg({'RMSE': ['mean']}).values[0][0]\n",
    "mean_MAE_best = df_results[df_results['regressor'] == 'Linear Regression'].agg({'MAE': ['mean']}).values[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As we can see, Linear Regression has the best performance in both RMSE and MAE. \n",
      "\n",
      "Mean RMSE is:\t 0.3915\n",
      "Mean MAE is:\t 0.2391\n"
     ]
    }
   ],
   "source": [
    "print(\"As we can see, Linear Regression has the best performance in both RMSE and MAE. \\n\\nMean RMSE is:\\t {:.4f}\".format(mean_RMSE_best))\n",
    "print(\"Mean MAE is:\\t {:.4f}\".format(mean_MAE_best))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refit baseline on train and predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set has years:  [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]\n",
      "The test set has years:  [2016]\n"
     ]
    }
   ],
   "source": [
    "print('The training set has years: ', list(df_train.Year.unique()))\n",
    "print('The test set has years: ', list(test.Year.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "train = df_train.sample(frac=1, random_state=SEED)\n",
    "test = test.sample(frac=1, random_state=SEED)\n",
    "\n",
    "# X and y\n",
    "X_train = train.drop(columns=['Y_maize_major','Year'], axis=1)\n",
    "y_train = train['Y_maize_major']\n",
    "X_test = test.drop(columns=['Y_maize_major','Year'], axis=1)\n",
    "y_test = test['Y_maize_major']\n",
    "\n",
    "# Scale to [0,1] range\n",
    "sc = MinMaxScaler()\n",
    "X_train = pd.DataFrame(sc.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(sc.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on train and predict on test\n",
    "reg = regressors['Linear Regression']\n",
    "reg.fit(X_train, y_train)\n",
    "y_test_pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test set (year 2016) (baseline results)\n",
      "RMSE: 0.3288\n",
      "MAE: 0.2161\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print metrics\n",
    "rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_test_pred)\n",
    "print('Performance on test set (year 2016) (baseline results)')\n",
    "print('RMSE:',round(rmse,4))\n",
    "print('MAE:',round(mae,4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_kursus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
