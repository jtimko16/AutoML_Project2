{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load prepared dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape: (32330, 52)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Countries</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>Sand_1</th>\n",
       "      <th>Sand_2</th>\n",
       "      <th>Sand_3</th>\n",
       "      <th>Sand_4</th>\n",
       "      <th>Sand_5</th>\n",
       "      <th>Sand_6</th>\n",
       "      <th>Sand_7</th>\n",
       "      <th>Clay_1</th>\n",
       "      <th>Clay_2</th>\n",
       "      <th>Clay_3</th>\n",
       "      <th>Clay_4</th>\n",
       "      <th>Clay_5</th>\n",
       "      <th>Clay_6</th>\n",
       "      <th>Clay_7</th>\n",
       "      <th>OC_1</th>\n",
       "      <th>OC_2</th>\n",
       "      <th>OC_3</th>\n",
       "      <th>OC_4</th>\n",
       "      <th>OC_5</th>\n",
       "      <th>OC_6</th>\n",
       "      <th>OC_7</th>\n",
       "      <th>PAW_1</th>\n",
       "      <th>PAW_2</th>\n",
       "      <th>PAW_3</th>\n",
       "      <th>PAW_4</th>\n",
       "      <th>PAW_5</th>\n",
       "      <th>PAW_6</th>\n",
       "      <th>PAW_7</th>\n",
       "      <th>Y_maize_major</th>\n",
       "      <th>Farm</th>\n",
       "      <th>Sow_Maize_month_int</th>\n",
       "      <th>Harvest_Maize_month_int</th>\n",
       "      <th>sow_to_harvest_months</th>\n",
       "      <th>maize_lag-1</th>\n",
       "      <th>pcp_mean_lag-1</th>\n",
       "      <th>tmax_mean_lag-1</th>\n",
       "      <th>tmin_mean_lag-1</th>\n",
       "      <th>spi_mean_lag-1</th>\n",
       "      <th>maize_lag-2</th>\n",
       "      <th>pcp_mean_lag-2</th>\n",
       "      <th>tmax_mean_lag-2</th>\n",
       "      <th>tmin_mean_lag-2</th>\n",
       "      <th>spi_mean_lag-2</th>\n",
       "      <th>maize_lag-3</th>\n",
       "      <th>pcp_mean_lag-3</th>\n",
       "      <th>tmax_mean_lag-3</th>\n",
       "      <th>tmin_mean_lag-3</th>\n",
       "      <th>spi_mean_lag-3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007</td>\n",
       "      <td>Angola</td>\n",
       "      <td>-8.75</td>\n",
       "      <td>14.75</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>48</td>\n",
       "      <td>45</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>37</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.615357</td>\n",
       "      <td>104_Angola</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.554392</td>\n",
       "      <td>97.103755</td>\n",
       "      <td>301.939623</td>\n",
       "      <td>292.214020</td>\n",
       "      <td>0.093447</td>\n",
       "      <td>0.721607</td>\n",
       "      <td>129.051864</td>\n",
       "      <td>301.518536</td>\n",
       "      <td>292.496579</td>\n",
       "      <td>1.644698</td>\n",
       "      <td>0.620005</td>\n",
       "      <td>109.983325</td>\n",
       "      <td>301.786056</td>\n",
       "      <td>292.204097</td>\n",
       "      <td>0.514275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007</td>\n",
       "      <td>Angola</td>\n",
       "      <td>-16.25</td>\n",
       "      <td>14.25</td>\n",
       "      <td>62</td>\n",
       "      <td>64</td>\n",
       "      <td>63</td>\n",
       "      <td>59</td>\n",
       "      <td>58</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.257656</td>\n",
       "      <td>99_Angola</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.117051</td>\n",
       "      <td>59.292237</td>\n",
       "      <td>301.882929</td>\n",
       "      <td>288.092753</td>\n",
       "      <td>0.182926</td>\n",
       "      <td>0.300217</td>\n",
       "      <td>47.697564</td>\n",
       "      <td>303.988747</td>\n",
       "      <td>288.916992</td>\n",
       "      <td>0.909295</td>\n",
       "      <td>0.212699</td>\n",
       "      <td>41.130026</td>\n",
       "      <td>303.298082</td>\n",
       "      <td>288.642853</td>\n",
       "      <td>0.588172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007</td>\n",
       "      <td>Angola</td>\n",
       "      <td>-17.25</td>\n",
       "      <td>14.25</td>\n",
       "      <td>69</td>\n",
       "      <td>71</td>\n",
       "      <td>70</td>\n",
       "      <td>67</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.286831</td>\n",
       "      <td>108_Angola</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3.093239</td>\n",
       "      <td>58.196545</td>\n",
       "      <td>302.891420</td>\n",
       "      <td>289.377311</td>\n",
       "      <td>0.991663</td>\n",
       "      <td>4.044452</td>\n",
       "      <td>42.130629</td>\n",
       "      <td>305.494178</td>\n",
       "      <td>290.535403</td>\n",
       "      <td>0.952237</td>\n",
       "      <td>2.295351</td>\n",
       "      <td>35.049776</td>\n",
       "      <td>304.824778</td>\n",
       "      <td>290.284886</td>\n",
       "      <td>0.371446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007</td>\n",
       "      <td>Angola</td>\n",
       "      <td>-11.75</td>\n",
       "      <td>14.75</td>\n",
       "      <td>60</td>\n",
       "      <td>63</td>\n",
       "      <td>61</td>\n",
       "      <td>57</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.700384</td>\n",
       "      <td>102_Angola</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.677797</td>\n",
       "      <td>149.210195</td>\n",
       "      <td>298.973795</td>\n",
       "      <td>287.311403</td>\n",
       "      <td>0.206751</td>\n",
       "      <td>0.907431</td>\n",
       "      <td>159.454723</td>\n",
       "      <td>299.404975</td>\n",
       "      <td>287.724299</td>\n",
       "      <td>1.374616</td>\n",
       "      <td>0.783018</td>\n",
       "      <td>174.088260</td>\n",
       "      <td>298.908208</td>\n",
       "      <td>287.362407</td>\n",
       "      <td>0.643207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007</td>\n",
       "      <td>Angola</td>\n",
       "      <td>-14.25</td>\n",
       "      <td>13.75</td>\n",
       "      <td>67</td>\n",
       "      <td>69</td>\n",
       "      <td>68</td>\n",
       "      <td>63</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.553450</td>\n",
       "      <td>43_Angola</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.412071</td>\n",
       "      <td>74.556629</td>\n",
       "      <td>304.006860</td>\n",
       "      <td>290.606725</td>\n",
       "      <td>-0.075621</td>\n",
       "      <td>0.675967</td>\n",
       "      <td>66.698670</td>\n",
       "      <td>304.644632</td>\n",
       "      <td>290.635254</td>\n",
       "      <td>1.144088</td>\n",
       "      <td>0.605584</td>\n",
       "      <td>67.404588</td>\n",
       "      <td>303.930955</td>\n",
       "      <td>290.564185</td>\n",
       "      <td>0.553079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year Countries    lat    lon  Sand_1  Sand_2  Sand_3  Sand_4  Sand_5  \\\n",
       "0  2007    Angola  -8.75  14.75      50      51      51      48      45   \n",
       "1  2007    Angola -16.25  14.25      62      64      63      59      58   \n",
       "2  2007    Angola -17.25  14.25      69      71      70      67      65   \n",
       "3  2007    Angola -11.75  14.75      60      63      61      57      53   \n",
       "4  2007    Angola -14.25  13.75      67      69      68      63      61   \n",
       "\n",
       "   Sand_6  Sand_7  Clay_1  Clay_2  Clay_3  Clay_4  Clay_5  Clay_6  Clay_7  \\\n",
       "0      46      46      37      35      36      39      42      42      42   \n",
       "1      59      59      27      25      26      29      31      30      30   \n",
       "2      65      66      19      16      18      21      24      24      23   \n",
       "3      53      53      29      26      28      32      35      36      36   \n",
       "4      61      61      22      19      21      25      28      28      29   \n",
       "\n",
       "   OC_1  OC_2  OC_3  OC_4  OC_5  OC_6  OC_7  PAW_1  PAW_2  PAW_3  PAW_4  \\\n",
       "0  0.52  0.23  0.17  0.09  0.04  0.02  0.02   0.15   0.15   0.14   0.13   \n",
       "1  0.11  0.05  0.07  0.04  0.02  0.02  0.01   0.11   0.10   0.10   0.09   \n",
       "2  0.09  0.06  0.07  0.04  0.02  0.02  0.02   0.10   0.10   0.10   0.09   \n",
       "3  0.46  0.16  0.14  0.08  0.05  0.04  0.03   0.12   0.13   0.12   0.12   \n",
       "4  0.15  0.09  0.09  0.05  0.02  0.01  0.01   0.11   0.11   0.11   0.11   \n",
       "\n",
       "   PAW_5  PAW_6  PAW_7  Y_maize_major        Farm  Sow_Maize_month_int  \\\n",
       "0   0.10   0.07   0.07       0.615357  104_Angola                    9   \n",
       "1   0.07   0.07   0.03       0.257656   99_Angola                    9   \n",
       "2   0.07   0.07   0.07       4.286831  108_Angola                    9   \n",
       "3   0.11   0.10   0.09       0.700384  102_Angola                    9   \n",
       "4   0.08   0.04   0.04       0.553450   43_Angola                    9   \n",
       "\n",
       "   Harvest_Maize_month_int  sow_to_harvest_months  maize_lag-1  \\\n",
       "0                        4                      7     0.554392   \n",
       "1                        4                      7     0.117051   \n",
       "2                        4                      7     3.093239   \n",
       "3                        4                      7     0.677797   \n",
       "4                        4                      7     0.412071   \n",
       "\n",
       "   pcp_mean_lag-1  tmax_mean_lag-1  tmin_mean_lag-1  spi_mean_lag-1  \\\n",
       "0       97.103755       301.939623       292.214020        0.093447   \n",
       "1       59.292237       301.882929       288.092753        0.182926   \n",
       "2       58.196545       302.891420       289.377311        0.991663   \n",
       "3      149.210195       298.973795       287.311403        0.206751   \n",
       "4       74.556629       304.006860       290.606725       -0.075621   \n",
       "\n",
       "   maize_lag-2  pcp_mean_lag-2  tmax_mean_lag-2  tmin_mean_lag-2  \\\n",
       "0     0.721607      129.051864       301.518536       292.496579   \n",
       "1     0.300217       47.697564       303.988747       288.916992   \n",
       "2     4.044452       42.130629       305.494178       290.535403   \n",
       "3     0.907431      159.454723       299.404975       287.724299   \n",
       "4     0.675967       66.698670       304.644632       290.635254   \n",
       "\n",
       "   spi_mean_lag-2  maize_lag-3  pcp_mean_lag-3  tmax_mean_lag-3  \\\n",
       "0        1.644698     0.620005      109.983325       301.786056   \n",
       "1        0.909295     0.212699       41.130026       303.298082   \n",
       "2        0.952237     2.295351       35.049776       304.824778   \n",
       "3        1.374616     0.783018      174.088260       298.908208   \n",
       "4        1.144088     0.605584       67.404588       303.930955   \n",
       "\n",
       "   tmin_mean_lag-3  spi_mean_lag-3  \n",
       "0       292.204097        0.514275  \n",
       "1       288.642853        0.588172  \n",
       "2       290.284886        0.371446  \n",
       "3       287.362407        0.643207  \n",
       "4       290.564185        0.553079  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/df_prepped.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print('df.shape:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Countries.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3883"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Farm.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year                       0\n",
       "Countries                  0\n",
       "lat                        0\n",
       "lon                        0\n",
       "Sand_1                     0\n",
       "Sand_2                     0\n",
       "Sand_3                     0\n",
       "Sand_4                     0\n",
       "Sand_5                     0\n",
       "Sand_6                     0\n",
       "Sand_7                     0\n",
       "Clay_1                     0\n",
       "Clay_2                     0\n",
       "Clay_3                     0\n",
       "Clay_4                     0\n",
       "Clay_5                     0\n",
       "Clay_6                     0\n",
       "Clay_7                     0\n",
       "OC_1                       0\n",
       "OC_2                       0\n",
       "OC_3                       0\n",
       "OC_4                       0\n",
       "OC_5                       0\n",
       "OC_6                       0\n",
       "OC_7                       0\n",
       "PAW_1                      0\n",
       "PAW_2                      0\n",
       "PAW_3                      0\n",
       "PAW_4                      0\n",
       "PAW_5                      0\n",
       "PAW_6                      0\n",
       "PAW_7                      0\n",
       "Y_maize_major              0\n",
       "Farm                       0\n",
       "Sow_Maize_month_int        0\n",
       "Harvest_Maize_month_int    0\n",
       "sow_to_harvest_months      0\n",
       "maize_lag-1                0\n",
       "pcp_mean_lag-1             0\n",
       "tmax_mean_lag-1            0\n",
       "tmin_mean_lag-1            0\n",
       "spi_mean_lag-1             0\n",
       "maize_lag-2                0\n",
       "pcp_mean_lag-2             0\n",
       "tmax_mean_lag-2            0\n",
       "tmin_mean_lag-2            0\n",
       "spi_mean_lag-2             0\n",
       "maize_lag-3                0\n",
       "pcp_mean_lag-3             0\n",
       "tmax_mean_lag-3            0\n",
       "tmin_mean_lag-3            0\n",
       "spi_mean_lag-3             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping countries and farms because one-hot-encoding would make 4000+ rows\n",
    "# We tested that and found that it did not improve performance but increased runtime\n",
    "df_label = df.loc[:,['Countries','Farm']]\n",
    "df = df.drop(['Countries','Farm'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate a test set, the year 2016\n",
    "test = df[df.Year == 2016]\n",
    "df_train = df[df.Year < 2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set has years:  [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]\n",
      "The test set has years:  [2016]\n"
     ]
    }
   ],
   "source": [
    "print('The training set has years: ', list(df_train.Year.unique()))\n",
    "print('The test set has years: ', list(test.Year.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline selection using CV and years 2007-2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  [2007, 2008, 2009, 2010] VAL:  [2011]\n",
      "TRAIN:  [2007, 2008, 2009, 2010, 2011] VAL:  [2012]\n",
      "TRAIN:  [2007, 2008, 2009, 2010, 2011, 2012] VAL:  [2013]\n",
      "TRAIN:  [2007, 2008, 2009, 2010, 2011, 2012, 2013] VAL:  [2014]\n",
      "TRAIN:  [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014] VAL:  [2015]\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation set-up\n",
    "# https://stackoverflow.com/questions/58069691/how-to-create-a-train-test-split-of-time-series-data-by-year\n",
    "\n",
    "year_list = df_train['Year'].unique().tolist()\n",
    "splits = {'train': [], 'val': []}\n",
    "\n",
    "for idx, yr in enumerate(year_list[:-1]):\n",
    "    if yr < 2010:\n",
    "        # To get only the last 5 splits\n",
    "        continue\n",
    "    train_yr = year_list[:idx+1]\n",
    "    test_yr = [year_list[idx+1]]\n",
    "    print('TRAIN: ', train_yr, 'VAL: ',test_yr)\n",
    "    \n",
    "    splits['train'].append(df_train.loc[df_train.Year.isin(train_yr), :])\n",
    "    splits['val'].append(df_train.loc[df_train.Year.isin(test_yr), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = {\n",
    "    \"KNN\": KNeighborsRegressor(), # 1 cannot extrapolate\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=SEED), # 2 cannot extrapolate\n",
    "    \"Gradient Boosting\": AdaBoostRegressor(random_state=SEED), # 3 cannot extrapolate\n",
    "    \"Linear Regression\": LinearRegression(), # 4 can extrapolate\n",
    "    \"LGBM\": lgb.sklearn.LGBMRegressor(random_state=SEED) # 5 can extrapolate\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "RMSE: 0.4227716309703176\n",
      "MAE: 0.2772231369475085\n",
      "Random Forest\n",
      "RMSE: 0.35504666930481626\n",
      "MAE: 0.23236377988207332\n",
      "Gradient Boosting\n",
      "RMSE: 0.49519943821403684\n",
      "MAE: 0.41166986202714034\n",
      "Linear Regression\n",
      "RMSE: 0.3821455005014209\n",
      "MAE: 0.25489443680604257\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005973 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5529\n",
      "[LightGBM] [Info] Number of data points in the train set: 15483, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 1.637678\n",
      "LGBM\n",
      "RMSE: 0.3932184676970676\n",
      "MAE: 0.2511848502701402\n",
      "###\n",
      "\n",
      "Fold:  2\n",
      "KNN\n",
      "RMSE: 0.4343983307394625\n",
      "MAE: 0.27719726810901024\n",
      "Random Forest\n",
      "RMSE: 0.3849396172418624\n",
      "MAE: 0.23210720357955217\n",
      "Gradient Boosting\n",
      "RMSE: 0.5424605719036462\n",
      "MAE: 0.40868128425372163\n",
      "Linear Regression\n",
      "RMSE: 0.392414424216494\n",
      "MAE: 0.23763037298284842\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5529\n",
      "[LightGBM] [Info] Number of data points in the train set: 18413, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 1.661491\n",
      "LGBM\n",
      "RMSE: 0.41897905633877314\n",
      "MAE: 0.2520347380126904\n",
      "###\n",
      "\n",
      "Fold:  3\n",
      "KNN\n",
      "RMSE: 0.41481088319377407\n",
      "MAE: 0.2756683630745393\n",
      "Random Forest\n",
      "RMSE: 0.3491609017376159\n",
      "MAE: 0.25143413144665927\n",
      "Gradient Boosting\n",
      "RMSE: 0.3922679268519174\n",
      "MAE: 0.3036205255414342\n",
      "Linear Regression\n",
      "RMSE: 0.3171278924411671\n",
      "MAE: 0.2409776608840372\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5529\n",
      "[LightGBM] [Info] Number of data points in the train set: 21343, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 1.670779\n",
      "LGBM\n",
      "RMSE: 0.3668401760628748\n",
      "MAE: 0.2512452054289344\n",
      "###\n",
      "\n",
      "Fold:  4\n",
      "KNN\n",
      "RMSE: 0.47786587274842274\n",
      "MAE: 0.3099127851975427\n",
      "Random Forest\n",
      "RMSE: 0.4250829231238818\n",
      "MAE: 0.2398800066548544\n",
      "Gradient Boosting\n",
      "RMSE: 0.4743615904394179\n",
      "MAE: 0.33351782067569896\n",
      "Linear Regression\n",
      "RMSE: 0.3815778477383467\n",
      "MAE: 0.21971599980708434\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006189 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5529\n",
      "[LightGBM] [Info] Number of data points in the train set: 24273, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 1.685119\n",
      "LGBM\n",
      "RMSE: 0.4111677601129646\n",
      "MAE: 0.2294958353692085\n",
      "###\n",
      "\n",
      "Fold:  5\n",
      "KNN\n",
      "RMSE: 0.5629285664946738\n",
      "MAE: 0.3297249379117888\n",
      "Random Forest\n",
      "RMSE: 0.5273907990107456\n",
      "MAE: 0.26960782515215237\n",
      "Gradient Boosting\n",
      "RMSE: 0.6267094431753109\n",
      "MAE: 0.4419091505086709\n",
      "Linear Regression\n",
      "RMSE: 0.5083432244453169\n",
      "MAE: 0.2684335056805056\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008042 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5529\n",
      "[LightGBM] [Info] Number of data points in the train set: 27203, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 1.709354\n",
      "LGBM\n",
      "RMSE: 0.5178689272862239\n",
      "MAE: 0.26414558450663267\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This will take some time\n",
    "\n",
    "reg_names = []\n",
    "cv_MAE = []\n",
    "cv_RMSE = []\n",
    "cv_folds = []\n",
    "train_years = []\n",
    "val_year = []\n",
    "\n",
    "i = 1\n",
    "for train, val in zip(splits['train'], splits['val']):\n",
    "\n",
    "    print('Fold: ', i)\n",
    "\n",
    "    # Shuffle\n",
    "    train = train.sample(frac=1, random_state=SEED)\n",
    "    val = val.sample(frac=1, random_state=SEED)\n",
    "    \n",
    "    # X and y\n",
    "    X_train = train.drop(columns=['Y_maize_major','Year'], axis=1)\n",
    "    y_train = train['Y_maize_major']\n",
    "    X_val = val.drop(columns=['Y_maize_major','Year'], axis=1)\n",
    "    y_val = val['Y_maize_major']\n",
    "\n",
    "    # Scale to [0,1] range\n",
    "    sc = MinMaxScaler()\n",
    "    X_train = pd.DataFrame(sc.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_val = pd.DataFrame(sc.transform(X_val), columns=X_val.columns)\n",
    "\n",
    "    # Fit and predict\n",
    "    for reg_name, reg in regressors.items():\n",
    "        reg_names.append(reg_name)\n",
    "        reg.fit(X_train, y_train)\n",
    "        y_val_pred = reg.predict(X_val)\n",
    "        rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "        mae = mean_absolute_error(y_val, y_val_pred)\n",
    "        cv_MAE.append(mae)\n",
    "        cv_RMSE.append(rmse)\n",
    "        cv_folds.append(i)\n",
    "        train_years.append(list(train.Year.unique()))\n",
    "        val_year.append(list(val.Year.unique()))\n",
    "\n",
    "        print(reg_name)\n",
    "        print('RMSE:', rmse)\n",
    "        print('MAE:', mae)\n",
    "\n",
    "    i += 1\n",
    "    print('###')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>regressor</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>train_years</th>\n",
       "      <th>val_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.422772</td>\n",
       "      <td>0.277223</td>\n",
       "      <td>[2007, 2008, 2009, 2010]</td>\n",
       "      <td>[2011]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.355047</td>\n",
       "      <td>0.232364</td>\n",
       "      <td>[2007, 2008, 2009, 2010]</td>\n",
       "      <td>[2011]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.495199</td>\n",
       "      <td>0.411670</td>\n",
       "      <td>[2007, 2008, 2009, 2010]</td>\n",
       "      <td>[2011]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.382146</td>\n",
       "      <td>0.254894</td>\n",
       "      <td>[2007, 2008, 2009, 2010]</td>\n",
       "      <td>[2011]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.393218</td>\n",
       "      <td>0.251185</td>\n",
       "      <td>[2007, 2008, 2009, 2010]</td>\n",
       "      <td>[2011]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.434398</td>\n",
       "      <td>0.277197</td>\n",
       "      <td>[2008, 2011, 2009, 2007, 2010]</td>\n",
       "      <td>[2012]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.384940</td>\n",
       "      <td>0.232107</td>\n",
       "      <td>[2008, 2011, 2009, 2007, 2010]</td>\n",
       "      <td>[2012]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.542461</td>\n",
       "      <td>0.408681</td>\n",
       "      <td>[2008, 2011, 2009, 2007, 2010]</td>\n",
       "      <td>[2012]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.392414</td>\n",
       "      <td>0.237630</td>\n",
       "      <td>[2008, 2011, 2009, 2007, 2010]</td>\n",
       "      <td>[2012]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.418979</td>\n",
       "      <td>0.252035</td>\n",
       "      <td>[2008, 2011, 2009, 2007, 2010]</td>\n",
       "      <td>[2012]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.414811</td>\n",
       "      <td>0.275668</td>\n",
       "      <td>[2011, 2008, 2009, 2010, 2012, 2007]</td>\n",
       "      <td>[2013]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.349161</td>\n",
       "      <td>0.251434</td>\n",
       "      <td>[2011, 2008, 2009, 2010, 2012, 2007]</td>\n",
       "      <td>[2013]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.392268</td>\n",
       "      <td>0.303621</td>\n",
       "      <td>[2011, 2008, 2009, 2010, 2012, 2007]</td>\n",
       "      <td>[2013]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.317128</td>\n",
       "      <td>0.240978</td>\n",
       "      <td>[2011, 2008, 2009, 2010, 2012, 2007]</td>\n",
       "      <td>[2013]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.366840</td>\n",
       "      <td>0.251245</td>\n",
       "      <td>[2011, 2008, 2009, 2010, 2012, 2007]</td>\n",
       "      <td>[2013]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.477866</td>\n",
       "      <td>0.309913</td>\n",
       "      <td>[2009, 2008, 2010, 2007, 2011, 2013, 2012]</td>\n",
       "      <td>[2014]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.425083</td>\n",
       "      <td>0.239880</td>\n",
       "      <td>[2009, 2008, 2010, 2007, 2011, 2013, 2012]</td>\n",
       "      <td>[2014]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.474362</td>\n",
       "      <td>0.333518</td>\n",
       "      <td>[2009, 2008, 2010, 2007, 2011, 2013, 2012]</td>\n",
       "      <td>[2014]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.381578</td>\n",
       "      <td>0.219716</td>\n",
       "      <td>[2009, 2008, 2010, 2007, 2011, 2013, 2012]</td>\n",
       "      <td>[2014]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.411168</td>\n",
       "      <td>0.229496</td>\n",
       "      <td>[2009, 2008, 2010, 2007, 2011, 2013, 2012]</td>\n",
       "      <td>[2014]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.562929</td>\n",
       "      <td>0.329725</td>\n",
       "      <td>[2009, 2013, 2010, 2011, 2007, 2012, 2014, 2008]</td>\n",
       "      <td>[2015]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.527391</td>\n",
       "      <td>0.269608</td>\n",
       "      <td>[2009, 2013, 2010, 2011, 2007, 2012, 2014, 2008]</td>\n",
       "      <td>[2015]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.626709</td>\n",
       "      <td>0.441909</td>\n",
       "      <td>[2009, 2013, 2010, 2011, 2007, 2012, 2014, 2008]</td>\n",
       "      <td>[2015]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.508343</td>\n",
       "      <td>0.268434</td>\n",
       "      <td>[2009, 2013, 2010, 2011, 2007, 2012, 2014, 2008]</td>\n",
       "      <td>[2015]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5</td>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.517869</td>\n",
       "      <td>0.264146</td>\n",
       "      <td>[2009, 2013, 2010, 2011, 2007, 2012, 2014, 2008]</td>\n",
       "      <td>[2015]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fold          regressor      RMSE       MAE  \\\n",
       "0      1                KNN  0.422772  0.277223   \n",
       "1      1      Random Forest  0.355047  0.232364   \n",
       "2      1  Gradient Boosting  0.495199  0.411670   \n",
       "3      1  Linear Regression  0.382146  0.254894   \n",
       "4      1               LGBM  0.393218  0.251185   \n",
       "5      2                KNN  0.434398  0.277197   \n",
       "6      2      Random Forest  0.384940  0.232107   \n",
       "7      2  Gradient Boosting  0.542461  0.408681   \n",
       "8      2  Linear Regression  0.392414  0.237630   \n",
       "9      2               LGBM  0.418979  0.252035   \n",
       "10     3                KNN  0.414811  0.275668   \n",
       "11     3      Random Forest  0.349161  0.251434   \n",
       "12     3  Gradient Boosting  0.392268  0.303621   \n",
       "13     3  Linear Regression  0.317128  0.240978   \n",
       "14     3               LGBM  0.366840  0.251245   \n",
       "15     4                KNN  0.477866  0.309913   \n",
       "16     4      Random Forest  0.425083  0.239880   \n",
       "17     4  Gradient Boosting  0.474362  0.333518   \n",
       "18     4  Linear Regression  0.381578  0.219716   \n",
       "19     4               LGBM  0.411168  0.229496   \n",
       "20     5                KNN  0.562929  0.329725   \n",
       "21     5      Random Forest  0.527391  0.269608   \n",
       "22     5  Gradient Boosting  0.626709  0.441909   \n",
       "23     5  Linear Regression  0.508343  0.268434   \n",
       "24     5               LGBM  0.517869  0.264146   \n",
       "\n",
       "                                         train_years val_year  \n",
       "0                           [2007, 2008, 2009, 2010]   [2011]  \n",
       "1                           [2007, 2008, 2009, 2010]   [2011]  \n",
       "2                           [2007, 2008, 2009, 2010]   [2011]  \n",
       "3                           [2007, 2008, 2009, 2010]   [2011]  \n",
       "4                           [2007, 2008, 2009, 2010]   [2011]  \n",
       "5                     [2008, 2011, 2009, 2007, 2010]   [2012]  \n",
       "6                     [2008, 2011, 2009, 2007, 2010]   [2012]  \n",
       "7                     [2008, 2011, 2009, 2007, 2010]   [2012]  \n",
       "8                     [2008, 2011, 2009, 2007, 2010]   [2012]  \n",
       "9                     [2008, 2011, 2009, 2007, 2010]   [2012]  \n",
       "10              [2011, 2008, 2009, 2010, 2012, 2007]   [2013]  \n",
       "11              [2011, 2008, 2009, 2010, 2012, 2007]   [2013]  \n",
       "12              [2011, 2008, 2009, 2010, 2012, 2007]   [2013]  \n",
       "13              [2011, 2008, 2009, 2010, 2012, 2007]   [2013]  \n",
       "14              [2011, 2008, 2009, 2010, 2012, 2007]   [2013]  \n",
       "15        [2009, 2008, 2010, 2007, 2011, 2013, 2012]   [2014]  \n",
       "16        [2009, 2008, 2010, 2007, 2011, 2013, 2012]   [2014]  \n",
       "17        [2009, 2008, 2010, 2007, 2011, 2013, 2012]   [2014]  \n",
       "18        [2009, 2008, 2010, 2007, 2011, 2013, 2012]   [2014]  \n",
       "19        [2009, 2008, 2010, 2007, 2011, 2013, 2012]   [2014]  \n",
       "20  [2009, 2013, 2010, 2011, 2007, 2012, 2014, 2008]   [2015]  \n",
       "21  [2009, 2013, 2010, 2011, 2007, 2012, 2014, 2008]   [2015]  \n",
       "22  [2009, 2013, 2010, 2011, 2007, 2012, 2014, 2008]   [2015]  \n",
       "23  [2009, 2013, 2010, 2011, 2007, 2012, 2014, 2008]   [2015]  \n",
       "24  [2009, 2013, 2010, 2011, 2007, 2012, 2014, 2008]   [2015]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame()\n",
    "df_results['fold'] = cv_folds\n",
    "df_results['regressor'] = reg_names\n",
    "df_results['RMSE'] = cv_RMSE\n",
    "df_results['MAE'] = cv_MAE\n",
    "df_results['train_years'] = train_years\n",
    "df_results['val_year'] = val_year\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv('../experiment_results/baseline_selection_results_Kea.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regressor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.506200</td>\n",
       "      <td>0.379880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.462555</td>\n",
       "      <td>0.293945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM</th>\n",
       "      <td>0.421615</td>\n",
       "      <td>0.249621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Regression</th>\n",
       "      <td>0.396322</td>\n",
       "      <td>0.244330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.408324</td>\n",
       "      <td>0.245079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       RMSE       MAE\n",
       "                       mean      mean\n",
       "regressor                            \n",
       "Gradient Boosting  0.506200  0.379880\n",
       "KNN                0.462555  0.293945\n",
       "LGBM               0.421615  0.249621\n",
       "Linear Regression  0.396322  0.244330\n",
       "Random Forest      0.408324  0.245079"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_agg = df_results.groupby('regressor').agg({'RMSE': ['mean'], 'MAE': [ 'mean']})\n",
    "df_results_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is has lowest RMSE and MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to find from results mean RMSE for Random Forest \n",
    "mean_RMSE_best = df_results[df_results['regressor'] == 'Linear Regression'].agg({'RMSE': ['mean']}).values[0][0]\n",
    "mean_MAE_best = df_results[df_results['regressor'] == 'Linear Regression'].agg({'MAE': ['mean']}).values[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As we can see, Linear Regression has the best performance in both RMSE and MAE. \n",
      "\n",
      "Mean RMSE is:\t 0.3963\n",
      "Mean MAE is:\t 0.2443\n"
     ]
    }
   ],
   "source": [
    "print(\"As we can see, Linear Regression has the best performance in both RMSE and MAE. \\n\\nMean RMSE is:\\t {:.4f}\".format(mean_RMSE_best))\n",
    "print(\"Mean MAE is:\\t {:.4f}\".format(mean_MAE_best))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refit baseline on train and predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set has years:  [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]\n",
      "The test set has years:  [2016]\n"
     ]
    }
   ],
   "source": [
    "print('The training set has years: ', list(df_train.Year.unique()))\n",
    "print('The test set has years: ', list(test.Year.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "train = df_train.sample(frac=1, random_state=SEED)\n",
    "test = test.sample(frac=1, random_state=SEED)\n",
    "\n",
    "# X and y\n",
    "X_train = train.drop(columns=['Y_maize_major','Year'], axis=1)\n",
    "y_train = train['Y_maize_major']\n",
    "X_test = test.drop(columns=['Y_maize_major','Year'], axis=1)\n",
    "y_test = test['Y_maize_major']\n",
    "\n",
    "# Scale to [0,1] range\n",
    "sc = MinMaxScaler()\n",
    "X_train = pd.DataFrame(sc.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(sc.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on train and predict on test\n",
    "reg = regressors['Linear Regression']\n",
    "reg.fit(X_train, y_train)\n",
    "y_test_pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test set (year 2016) (baseline results)\n",
      "RMSE: 0.33\n",
      "MAE: 0.213\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print metrics\n",
    "rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_test_pred)\n",
    "print('Performance on test set (year 2016) (baseline results)')\n",
    "print('RMSE:',round(rmse,4))\n",
    "print('MAE:',round(mae,4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_kursus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
